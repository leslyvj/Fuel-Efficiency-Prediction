# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJBHpA3J-qID7Va4ELNF23qAodUypyiq

## **Predicting Fuel efficiency Using Multiple Linear Regression**

# Import Necessary Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE

"""# Load Dataset"""

df = pd.read_csv('/content/cars.csv')
df.head()

"""# Describing the Dataset"""

df.describe()

df.shape

"""# **Finding the Number of Observations (Instances) in the Dataset**"""

df.shape[0]

"""# **Finding Numerical Variables in the Dataset and list them**"""

#list numerical variables
numerical_cols=df.select_dtypes(include=[np.number]).columns
numerical_cols

"""# **Finding Categorical Variables in the Dataset  and list them**"""

#list categorical variable
categorical_variables=df.select_dtypes(include=['object']).columns
print(categorical_variables)

"""# Data Types in the data"""

df.dtypes

"""# **Data Cleaning**"""

df.duplicated().sum()

"""number of duplicated rows=18"""

df.drop_duplicates(inplace=True)

df.duplicated().sum()

"""number of duplicated rows=0

# Handle Outliers
"""

# Check for outliers in numerical columns
for col in numerical_cols:
    # Example using IQR method for outlier detection
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
print(df.shape)

"""# Handle Categorical Variables"""

#Handle categorical variables
df.drop(['Identification.ID'],axis=1,inplace=True)

# Assuming the target variable is 'Fuel Information.City mpg'
target = 'Fuel Information.City mpg'

# Columns to target encode
target_encoding_cols = ['Identification.Make', 'Identification.Model Year', 'Engine Information.Engine Type','Engine Information.Driveline']

# Perform target encoding
for col in target_encoding_cols:
    # Calculate the mean target value for each category
    target_mean = df.groupby(col)[target].mean()
    # Map the mean to each category in the dataset
    df[col + '_encoded'] = df[col].map(target_mean)

# Display the first few rows to inspect the result
df[[col + '_encoded' for col in target_encoding_cols]].head()

df.head()

df.drop(target_encoding_cols,axis=1,inplace=True)

df.shape

# Columns for One-Hot Encoding
one_hot_encoding_cols = [
    'Fuel Information.Fuel Type',
    'Engine Information.Transmission',
    'Identification.Classification'
]

# Perform One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=one_hot_encoding_cols, drop_first=True)

df_encoded.head()

"""# R² before feature selection"""

# Define target and features (X = all features, y = target variable)
X = df_encoded.drop(columns=["Fuel Information.City mpg"])
y = df_encoded["Fuel Information.City mpg"]

# Split the data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model with all features
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate R² score
r2 = r2_score(y_test, y_pred)

print(f"R² Score before feature selection: {r2:.4f}")

"""# Feature Scaling"""

#feature scaling
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
df_encoded[df_encoded.select_dtypes(include=[np.number]).columns]=scaler.fit_transform(df_encoded[df.select_dtypes(include=[np.number]).columns])
df_encoded.head()

df_encoded.dtypes

#correlation heatmap
plt.figure(figsize=(20,10))
sns.heatmap(df_encoded.corr(),annot=True)
plt.show()

"""# Feature Selection Using RFE"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import Ridge

X = df_encoded.drop('Fuel Information.City mpg', axis=1)
y = df_encoded['Fuel Information.City mpg']

ridge = Ridge()
rfe = RFE(ridge, n_features_to_select=5)  # Select top 5 features
rfe.fit(X, y)

selected_features = X.columns[rfe.support_]
print("Selected Features by RFE:", selected_features)

#convert boolean column to numeric
bool_cols = df_encoded.select_dtypes(include=['bool']).columns
df_encoded[bool_cols] = df_encoded[bool_cols].astype(int)

"""# Linearity"""

#scatterplot for linear realtionship
sns.scatterplot(x='Engine Information.Engine Statistics.Horsepower',y='Fuel Information.City mpg',data=df_encoded)

sns.scatterplot(x='Fuel Information.Highway mpg',y='Fuel Information.City mpg',data=df_encoded)

sns.scatterplot(x='Identification.Model Year_encoded',y='Fuel Information.City mpg',data=df_encoded)

sns.scatterplot(x='Engine Information.Engine Type_encoded',y='Fuel Information.City mpg',data=df_encoded)

sns.scatterplot(x='Fuel Information.Fuel Type_E85',y='Fuel Information.City mpg',data=df_encoded)

"""# Multicolinearity"""

#vif
from statsmodels.stats.outliers_influence import variance_inflation_factor
selected_features = ['Fuel Information.Highway mpg',
       'Engine Information.Engine Statistics.Horsepower',
       'Identification.Model Year_encoded',
       'Engine Information.Engine Type_encoded',
       'Fuel Information.Fuel Type_E85']

X = df_encoded[selected_features]

# Create a DataFrame to store VIF values
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

from sklearn.decomposition import PCA
# Extract features with high VIF
high_vif_features = ['Fuel Information.Highway mpg', 'Identification.Model Year_encoded','Engine Information.Engine Type_encoded']


# Apply PCA
pca = PCA(n_components=1)  # Reduce to a single component
X_pca = pca.fit_transform(df_encoded[high_vif_features])

# Create a new column in the DataFrame for the PCA component
df_encoded['PCA_Fuel_ModelYear'] = X_pca

# Drop the original correlated features
df_encoded = df_encoded.drop(columns=high_vif_features)

print("Transformed DataFrame with PCA feature added:")
df_encoded.head()

# VIF after PCA on selected features
X = df_encoded[['Engine Information.Engine Statistics.Horsepower', 'Fuel Information.Fuel Type_E85', 'PCA_Fuel_ModelYear']]

# Create a DataFrame to store VIF values
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

vif_data

"""# Model Training"""

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

#multiple linear regression
model=LinearRegression()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
y_pred

"""# Model Evaluation"""

#R-Squared Evaluation
r2_score(y_test,y_pred)

err=y_test-y_pred
err

"""# Distribution"""

#Residual Plot
sns.distplot(err,color='blue')
plt.axvline(x=0,color='r',linestyle='--')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Plot')
plt.show()

sns.scatterplot(x=y_pred,y=err)
plt.axhline(y=0,color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()